{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACM SIGCHI Summer School on Computational Interaction  \n",
    "### Inference, optimization and modeling for the engineering of interactive systems  \n",
    "#### 27th August - 1st September 2018  \n",
    "#### University of Cambridge, UK  \n",
    "\n",
    "<img src=\"imgs/logo.png\" width=\"10%\">\n",
    "\n",
    "$$\n",
    "\\newcommand{\\vec}[1]{{\\bf #1} } \n",
    "\\newcommand{\\real}{\\mathbb{R}}\n",
    "\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\vec{x}\n",
    "\\real$$\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# Introduction to unsupervised learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Some mathematical notation\n",
    "\n",
    "We will by considering datasets which consist of a series of measurements. We learn from a *training set* of data.\n",
    "Each measurement is called a *sample* or *datapoint*, and each measurement type is called a *feature*. \n",
    "\n",
    "If we have $n$ samples and $d$ features, we form a matrix $X$ of size $n \\times d$, which has $n$ rows of $d$ measurements. $d$ is the **dimension** of the measurements. $n$ is the **sample size**.  Each row of $X$ is called a *feature vector*. For example, we might have 200 images of digits, each of which is a sequence of $8\\times8=64$ measurements of brightness, giving us a $200 \\times 64$ dataset. The rows of image values are the *features*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as axes3d\n",
    "import sklearn.preprocessing, sklearn.cluster,  sklearn.neighbors\n",
    "import sklearn.metrics, sklearn.datasets, sklearn.decomposition, sklearn.manifold\n",
    "import pandas as pd\n",
    "\n",
    "# force plots to appear inline on this page\n",
    "%matplotlib notebook\n",
    "plt.rc('figure', figsize=(8.0, 4.0), dpi=140)\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "Supervised learning involves learning a relationship between attribute variables and target variables; in other words learning a function which maps input measurements to target values. This can be in the context of making discrete decisions (is this image a car or not?) or learning continuous relationships (how loud will this aircraft wing be if I make the shape like this?).\n",
    "\n",
    "## Unsupervised learning\n",
    "Unsupervised learning learns the structure of data without any explicit labeling of points. The key idea is that datasets may have a simple underlying or *latent* representation which can be determined simply by looking at the data itself.\n",
    "\n",
    "Two common unsupervised learning tasks are *clustering* and *dimensional reduction*. Clustering can be thought of as the unsupervised analogue of classification -- finding discrete classes in data. Dimensional reduction can be thought of as the analogue of regression -- finding a small set of continuous variables which \"explain\" a higher dimensional set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "One very simple method of dimensional reduction is *principal component analysis*. This is a linear method; in other words it finds rigid rotations and scalings of the data to project it onto a lower dimension. That is, it finds a matrix $A$ such that $y=Ax$ gives a mapping from $d$ dimensional $x$ to $d^\\prime$ dimensional $y$.\n",
    "\n",
    "The PCA algorithm effectively looks for the rotation that makes the dataset look \"fattest\" (maximises the variance), chooses that as the first dimension, then removes that dimension, rotates again to make it look \"fattest\" and repeats. Linear algebra makes it efficient to do this process in a single step by extracting the *eigenvectors* of the *covariance matrix*. \n",
    "\n",
    "PCA always finds a matrix $A$ such that $y = Ax$, where the dimension of $y<x$. PCA is exact and repeatable and very efficient, but it can only find rigid transformations of the data. This is a limitation of any linear dimensional reduction technique.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits()\n",
    "digit_data = digits.data\n",
    "\n",
    "# plot a single digit data element\n",
    "def show_digit(d):\n",
    "    plt.imshow(d.reshape(8,8), cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.figure()\n",
    "    \n",
    "# show a couple of raw digits\n",
    "show_digit(digit_data[0])\n",
    "show_digit(digit_data[400])\n",
    "\n",
    "# apply principal component analysis\n",
    "pca = sklearn.decomposition.PCA(n_components=2).fit(digit_data)\n",
    "digits_2d = pca.transform(digit_data)\n",
    "\n",
    "# plot each digit with a different color\n",
    "plt.scatter(digits_2d[:,0], digits_2d[:,1], c=digits.target, cmap='rainbow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful property of PCA is that we compute exactly how \"fat\" each of these learned dimensions were. The ratio of *explained variance* tells us how much each of the original variation in the dataset is captured by each learned dimension. \n",
    "\n",
    "If most of the variance is in the first couple of components, we know that a 2D representation will capture much of the original dataset. If the ratios of variance are spread out over many dimensions, we will need many dimensions to represent the data well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see how many dimensions we need to represent the data well using the eigenspectrum\n",
    "# here we show the first 32 components\n",
    "pca = sklearn.decomposition.PCA(n_components=32).fit(digit_data)\n",
    "plt.bar(np.arange(32), pca.explained_variance_ratio_)\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Proportion of variance explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example where PCA does badly is the \"swiss roll dataset\" -- a plane rolled up into a spiral in 3D. This has a very simple structure; a simple plane with some distortion. But PCA can never unravel the spiral to find this simple explanation because it cannot be unravelled via a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_pos, swiss_val = sklearn.datasets.make_swiss_roll(800, noise=0.0)\n",
    "fig = plt.figure()\n",
    "# make a 3D figure\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(swiss_pos[:,0], swiss_pos[:,1], swiss_pos[:,2], c=swiss_val, cmap='gist_heat', s=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to learn this structure (which doesn't help much)\n",
    "pca = sklearn.decomposition.PCA(2).fit(swiss_pos)\n",
    "pca_pos = pca.transform(swiss_pos)\n",
    "plt.scatter(pca_pos[:,0], pca_pos[:,1], c=swiss_val, cmap='gist_heat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold learning\n",
    "Other approaches to dimensional reduction look at the problem in terms of learning a *manifold*. A *manifold* is a geometrical structure which is locally like a low-dimensional Euclidean space. Examples are the plane rolled up in the swiss roll, or a 1D \"string\" tangled up in a 3D space. \n",
    "\n",
    "Manifold approaches attempt to automatically find these smooth embedded structures by examining the local structure of datapoints (often by analysing the nearest neighbour graph of points). This is more flexible than linear dimensional reduction as it can in theory unravel very complex or tangled datasets. \n",
    "\n",
    "However, the algorithms are usually approximate, they do not give guarantees that they will find a given manifold, and can be computationally intensive to run.\n",
    "\n",
    "A popular manifold learning algorithm is *ISOMAP* which uses nearest neighbour graphs to identify locally connected parts of a dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap_pos = sklearn.manifold.Isomap(n_neighbors=10, n_components=2).fit_transform(swiss_pos)\n",
    "plt.scatter(isomap_pos[:,0], isomap_pos[:,1], c=swiss_val, cmap='gist_heat')\n",
    "plt.figure()\n",
    "\n",
    "# note that isomap is sensitive to noise!\n",
    "noisy_swiss_pos, swiss_val = sklearn.datasets.make_swiss_roll(800, noise=0.5)\n",
    "isomap_pos = sklearn.manifold.Isomap(n_neighbors=10, n_components=2).fit_transform(noisy_swiss_pos)\n",
    "plt.scatter(isomap_pos[:,0], isomap_pos[:,1], c=swiss_val, cmap='gist_heat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a more modern dimensional reduction method -- t-distributed stochastic neighbour embedding -- to the digits dataset we used in the PCA example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very slow to run\n",
    "tsne_digits = sklearn.manifold.TSNE(n_components=2).fit_transform(digit_data)\n",
    "plt.scatter(tsne_digits[:,0], tsne_digits[:,1], c=digits.target, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE separates out the digits very nicely into separate regions one for each digit, from the image data alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
