{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ACM SIGCHI Summer School on Computational Interaction  \n",
    "Inference, optimization and modeling for the engineering of interactive systems  \n",
    "27th August - 1st September 2018  \n",
    "University of Cambridge, UK  ](imgs/logo_full.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Learning control manifolds: data-driven HCI\n",
    "$$\n",
    "\\newcommand{\\vec}[1]{{\\bf #1} } \n",
    "\\newcommand{\\real}{\\mathbb{R}}\n",
    "\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\vec{x}\n",
    "\\real$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline \n",
    "\n",
    "### Modelling sensor vector streams\n",
    "* <a href=\"#unsupervised\"> Discuss computational approaches to modelling sensors as input devices </a>\n",
    "* <a href=\"#inherent\">Discuss the ideas of inherent structures in sensor streams </a>\n",
    "* <a href=\"#keyboard\"> Look at the keyboard from a different perspective </a>\n",
    "* <a href=\"#capturing\"> Try out the Rewarding the Original\" process for analysing sensor streams\n",
    "\n",
    "### Unsupervised learning\n",
    "* <a href=\"#notation\"> Introduce standard notation for machine learning </a>\n",
    "* <a href=\"#clustering\"> Discuss basic clustering </a>\n",
    "\n",
    "### Clustering and manifold learning\n",
    "* <a href=\"#clustering_ex\"> Try out clustering to cluster images in pixel space.\n",
    "* <a href=\"#manifold\"> Introduce manifold learning, along with PCA and self-organising maps.\n",
    "* <a href=\"#manifold_hci\"> Show how the keyboard can be unraveled by unsupervised learning.\n",
    "* <a href=\"#beard_pointer\"> Look at how ISOMAP can build a beard pointer without supervision.\n",
    "* <a href=\"#practical\"> Challenge: implement unsupervised learning from a camera feed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unsupervised\"> </a>\n",
    "## Unsupervised learning for sensor data\n",
    "In this part, we will explore how **unsupervised learning** (and semi-supervised learning) can pull out structure from **sensors**. We can use this \"natural\", latent structure to build interfaces without having to predefine our controls. This will gives a substrate which we can then attach to useful actions, in the knowledge that what we are designing is based on empirically derived control features.\n",
    "\n",
    "### Why is this computational interaction?\n",
    "Unsupervised learning learns a model of interaction directly from **data**. The way in which user action is interpreted is determined empirically through a rigorous algorithmic process. This is both a **computational process** for capturing user behaviour systematically and mapping it on actions, and actionable computational models that can be applied to specific problems.\n",
    "\n",
    "We **do not** start by designing an algorithm to recognise inputs. Instead we *computationally* analyse inputs and derive an algorithm from this analysis. \n",
    "\n",
    "#### Properties\n",
    "The unsupervised learning approach is:\n",
    "\n",
    "| Property | Why  |\n",
    "|----------|------|\n",
    "|**data-driven**  |  Captures how we should interact by observing how we can interact   |\n",
    "|**generalisable**  |  Makes weak assumptions about the nature of interaction. |\n",
    "|**quantitative** | Provides numerical tools to design and evaluate the *design process*. |\n",
    "|**objective**| Provides an analytical base before assumptions and idioms are introduced. |\n",
    "\n",
    "\n",
    "### Motivation\n",
    "<a id=\"motivation\"></a>\n",
    "<img src=\"imgs/mainfold_labeled.png\">\n",
    "\n",
    "For many conventional UI sensors, we already have good mappings from **sensor measurements** to **interface actions**. This is largely because the sensors were designed specifically to have electrical outputs which are very close to the intended actions; a traditional mechanical mouse literally emits electrical pulses at a rate proportional to translation.\n",
    "\n",
    "But with optical sensors like a Kinect, or with a high-degree of freedom flexible sensor, or tricky sensors like electromyography (which measures the electrical signals present as muscles contract), these mappings become tricky. Supervised learning lets train a system to recognise patterns in these signals (e.g. to classify poses or gestures). \n",
    "**But what if you don't know what's even feasible or would make a good interface?**\n",
    "\n",
    "## Inherent structures\n",
    "\n",
    "If we take sensor measurements of a person doing \"random stuff\" (derived from [Rewarding the Original](http://www.dcs.gla.ac.uk/~jhw/motionexplorerdata/) CHI 2012 for ideas on how to make \"random stuff\" a formal process), we will will end up with a set of feature vectors that were both **performable** and **measurable** (because we know someone did them and a sensor measured them). \n",
    "\n",
    "\n",
    "\n",
    "One way to look at this data is to recover **inherent structure** in these measurements. We can ask some pertinent questions:\n",
    "* are there **regularities** or **stable points** which represent things which might be good controls? \n",
    "    * Can we find these empirically? \n",
    "* Can we link stable points to useful actions we want to be able to do? \n",
    "* Can we infer user intentions from these stable points robustly?\n",
    "\n",
    "\n",
    "<a href=\"https://www.youtube.com/embed/tNQJHWVB_QA\"> <img src=\"imgs/motion_video_frame.png\"> </a>\n",
    "\n",
    "# Sampling inherent structures\n",
    "\n",
    "Desiderata:\n",
    "* find control opportunities with minimal assumptions about sensors\n",
    "* capture a parsimonious space for interaction -- only that which can be done and can be sensed\n",
    "* efficiently and reproducibly capture interactions\n",
    "* map out characterstics of sensor vectors\n",
    "\n",
    "<img src=\"imgs/map.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"keyboard\"> </a>\n",
    "## Making the familiar unfamiliar: keyboard vectors\n",
    "This code will display a window. The output will change as keys are pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "plt.rc('figure', figsize=(8.0, 4.0), dpi=180)\n",
    "import scipy.stats\n",
    "import pykalman\n",
    "from scipy.stats import norm\n",
    "import scipy.stats\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from key_display import key_tk\n",
    "import keyboard\n",
    "state = key_tk()\n",
    "%gui tk\n",
    "keyboard.restore_state(state)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input as a stream of vectors\n",
    "What is being visualised? The output is treating the keyboard as a 128 dimensional binary vector; **a point in $\\real^{128}$** for each time $t$.\n",
    "\n",
    "As keys go and up and down, they switch on and off the relevant elements of the vector. This vector has some process noise and a bit of temporal smoothing applied.  The order of the elements is random but fixed. \n",
    "\n",
    "This is an unfamiliar way of looking at a keyboard input, where we might expect to consume key information asynchronously from an event loop, and key events would come as fully formed data structures.\n",
    "\n",
    "However, this is typical for sensors that might be encountered:\n",
    "* There is **noise**, or uncertainty in measurement.\n",
    "* There is a **very high dimension** of state measured, but a low dimension of control exerted.\n",
    "* There are **continuous dynamics**; instantaneous changes of state are not possible.\n",
    "* Data comes as a **regular array**; but without much more structure than that.\n",
    "* Input comes **synchronously**, as a sampled stream.\n",
    "\n",
    "The \"ordinary\" keyboard input is a highly massaged, processed version of raw input (not that the visualised vector version is an authentic representation of the raw input either).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"capturing\"><a>\n",
    "## Rewarding the Original: Capturing the repertoire\n",
    "We can run the code again, and use a simplified rewarding the original algorithm to capture the \"interesting\" vectors.\n",
    "\n",
    "At the bottom, there is a count shown. This is a count of the number of *unique vectors* (for some sense of unique) seen so far. Try pressing a key a few times; the counter will increase then stop increasing.\n",
    "\n",
    "A collection of vectors is being sampled as the process runs. This *repertoire* is augmented with a new input if the input is different enough from that seen before. This allows to collect all of the distinct vectors that this user/input device combination is capable of generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_list = {\"data\":[], \"time\":[]} # this will hold the collected vectors\n",
    "state = key_tk(rwo_kwargs={\"bag\":vec_list, \"threshold\":0.42, \"metric\":'euclidean'}, alpha=0.8)\n",
    "%gui tk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboard.restore_state(state)  \n",
    "try:\n",
    "    os.mkdir(\"captured_data\")\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# save the state to a file\n",
    "fname = \"captured_data/rwo_{0}.npy\".format(time.asctime().replace(\" \", \"_\").replace(\":\", \"_\"))\n",
    "print(fname)\n",
    "np.save(fname, np.array(vec_list[\"data\"]))\n",
    "print(np.load(fname).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing the vectors\n",
    "We can view this as a matrix, showing each captured vector as a row, in time order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(vec_list[\"data\"]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring results\n",
    "\n",
    "We can use these results to capture something about \"how much\" of the possible space is explored. For example, we could compare the diversity of results using a single finger running over the keyboard to a fist running over the keyboard.\n",
    "\n",
    "Some simple measures are:\n",
    "* the number of vectors generated in total\n",
    "* the \"invention rate\"; number of vectors per second\n",
    "* the \"volume\" of the space that is explored (computed as the log-determinant of the covariance matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. vectors: {0}\".format(len(vec_list[\"data\"])))\n",
    "print(\"Volume: {0:.2e}\".format(np.log(np.linalg.det(np.cov(vec_list[\"data\"])))))\n",
    "print(\"Median invention rate: {0:.2f} vecs/second\".format(np.median(np.diff(vec_list[\"time\"]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical: Mash your keyboard\n",
    "Try capturing two datasets. One precise (e.g. dragging a single finger) and one less precise (e.g. dragging a fist) over the surface.\n",
    "See if you can exhaust the capture when `threshold=0.4`.\n",
    "\n",
    "Compare the statistics for the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewarding the original\n",
    "\n",
    "In summary:\n",
    "\n",
    "* We have a process for capturing sensor vectors that correspond to possible and measurable states of a sensing device.\n",
    "* We can analyse the results comparing possibilities with different users, sensors and tasks, including measures such as total number of vectors, \"volume\", \"overlap\", \"time of invention\" and so on.\n",
    "* The set we capture can be conditioned on specific variables (e.g. those vectors generated by a whole wrist, or just one finger, or while holding a coffee cup (don't try this!))\n",
    "* We can look at many types of input as streams of vectors, and process these using standard machine learning tools.\n",
    "* **However** this proces does not distinguish noise from intentional control. A noisy sensor looks more \"innovative\" than a clean one. These metrics are useful relative comparisons, but must be treated cautiously.\n",
    "\n",
    "\n",
    "### From analysis to synthesis\n",
    "This is a powerful *analytic* tool, which we could use to analyse potential input devices or the effect of user impairments on interaction. It would be nice to be able to use these ideas for interaction *synthesis* as well. \n",
    "\n",
    "But one problem is that we end up with a collection of *very* high dimensional vectors. This is hard to work with -- how we would use the information captured to design?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"notation\"> </a>\n",
    "## Some mathematical notation\n",
    "\n",
    "We will by considering datasets which consist of a series of measurements. We learn from a *training set* of data.\n",
    "Each measurement is called a *sample* or *datapoint*, and each measurement type is called a *feature*. \n",
    "\n",
    "If we have $n$ samples and $d$ features, we form a matrix $X$ of size $n \\times d$, which has $n$ rows of $d$ measurements. $d$ is the **dimension** of the measurements. $n$ is the **sample size**.  Each row of $X$ is called a *feature vector*. For example, we might have 200 images of digits, each of which is a sequence of $8\\times8=64$ measurements of brightness, giving us a $200 \\times 64$ dataset. The rows of image values are the *features*.\n",
    "\n",
    "### Geometry of feature vectors\n",
    "Each feature vector is a point in an $\\real^d$ space. Typically the ordering of $n$ samples is not relevant; the information is represented **geometrically**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold, sklearn.cluster, sklearn.datasets, sklearn.decomposition\n",
    "import scipy.stats\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "Supervised learning involves learning a relationship between attribute variables and target variables; in other words learning a function which maps input measurements to target values. This can be in the context of making discrete decisions (is this image a car or not?) or learning continuous relationships (how loud will this aircraft wing be if I make the shape like this?). Most, but not all, common machine learning problems are framed as supervised learning problems.\n",
    "\n",
    "We're going to focus on **unsupervised** learning for the rest of this section.\n",
    "\n",
    "## Unsupervised learning\n",
    "Unsupervised learning learns \"interesting things\" about the structure of data without any explicit labeling of points. The key idea is that datasets may have a simple underlying or *latent* representation which can be determined simply by looking at the data itself.\n",
    "\n",
    "Two common unsupervised learning tasks are *clustering* and *dimensional reduction*. Clustering can be thought of as the unsupervised analogue of classification -- finding discrete classes in data. Dimensional reduction can be thought of as the analogue of regression -- finding a small set of continuous variables which \"explain\" a higher dimensional set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering\">\n",
    "### Clustering\n",
    "\n",
    "Clustering tries to find well-seperated (in some sense) **partitions** of a data set. It is essentially a search for natural boundaries in the data. \n",
    "\n",
    "\n",
    "<img src=\"imgs/cluster_img.png\">\n",
    "\n",
    "There are many, *many* clustering approaches. A simple one is *k-means*, which finds clusters via an iterative algortihm. The number of clusters must be chosen in advance. In general, it is hard to estimate the number of clusters, although there are algorithms for estimating this. k-means proceeds by choosing a set of $k$ random points as initial cluster seed points; classifiying each data point according to its nearest seed point; then moving the cluster point towards the mean position of all the data points that belong to it. \n",
    "\n",
    "We can use this to find *dense, disconnected* regions of a dataset. In a sensor stream example, this might be a sequence of sensor inputs that occur commonly because they represent a particular state. A simple switch, for example, could be measured as a sampled signal indicating resistance. Although there would be some (very little) noise, there would be two clear clusters corresponding to the on and off states.\n",
    "\n",
    "The k-means algorithm does not guarantee to find the best possible clustering -- it falls into *local minima*. But it often works very well.\n",
    "\n",
    "<img src=\"imgs/cluster_boundary.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits()\n",
    "digit_data = digits.data\n",
    "\n",
    "\n",
    "selection = np.random.randint(0,200,(10,))\n",
    "\n",
    "digit_seq = [digit_data[s].reshape(8,8) for s in selection]\n",
    "plt.imshow(np.hstack(digit_seq), cmap=\"gray\", interpolation=\"nearest\")\n",
    "for i, d in enumerate(selection):    \n",
    "    plt.text(4+8*i,10,\"%s\"%digits.target[d])\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Some random digits from the downscaled MNIST set\")\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply principal component analysis\n",
    "pca = sklearn.decomposition.PCA(n_components=2).fit(digit_data)\n",
    "digits_2d = pca.transform(digit_data)\n",
    "\n",
    "# plot each digit with a different color (these are the true labels)\n",
    "plt.scatter(digits_2d[:,0], digits_2d[:,1], c=digits.target, cmap='jet', s=6)\n",
    "plt.title(\"A 2D plot of the digits, colored by true label\")\n",
    "# show a few random draws from the examples, and their labels\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now cluster the data\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=10)\n",
    "kmeans_target = kmeans.fit_predict(digits.data)\n",
    "plt.scatter(digits_2d[:,0], digits_2d[:,1], c=kmeans_target, cmap='jet', s=6)\n",
    "plt.title(\"Points colored by cluster inferred\")\n",
    "\n",
    "# plot some items in the same cluster\n",
    "# (which should be the same digit or similar!)\n",
    "def plot_same_target(target):\n",
    "    plt.figure()\n",
    "    selection = np.where(kmeans_target==target)[0][0:20]\n",
    "    digit_seq = [digit_data[s].reshape(8,8) for s in selection]\n",
    "    plt.imshow(np.hstack(digit_seq), cmap=\"gray\", interpolation=\"nearest\")\n",
    "    for i, d in enumerate(selection):    \n",
    "        plt.text(4+8*i,10,\"%s\"%digits.target[d])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Images from cluster %d\" % target)\n",
    "    \n",
    "for i in range(10):    \n",
    "    plot_same_target(i)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now cluster the data, but do it with too few and too many clusters\n",
    "\n",
    "for clusters in [3,20]:\n",
    "    plt.figure()\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=clusters)\n",
    "    kmeans_target = kmeans.fit_predict(digits.data)\n",
    "    plt.scatter(digits_2d[:,0], digits_2d[:,1], c=kmeans_target, cmap='jet')\n",
    "    plt.title(\"%d clusters is not good\" % clusters)\n",
    "    # plot some items in the same cluster\n",
    "    # (which should be the same digit or similar!)\n",
    "    def plot_same_target(target):\n",
    "        plt.figure()\n",
    "        selection = np.random.permutation(np.where(kmeans_target==target))[0][0:20]\n",
    "        digit_seq = [digit_data[s].reshape(8,8) for s in selection]\n",
    "        plt.imshow(np.hstack(digit_seq), cmap=\"gray\", interpolation=\"nearest\")\n",
    "        for i, d in enumerate(selection):    \n",
    "            plt.text(4+8*i,10,\"%s\"%digits.target[d])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    for i in range(clusters):\n",
    "        plot_same_target(i)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical: Day and night\n",
    "<a id=\"clustering_ex\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a clustering algorithm (choose one from [sklearn](http://scikit-learn.org/stable/modules/clustering.html#clustering)) to cluster a set of images of street footage, some filmed at night, some during the day.\n",
    "\n",
    "The images are available by loading `data/daynight.npz` using `np.load()`. This is a has 512 images of size 160x65, RGB color, 8-bit unsigned integer. You can access these as:\n",
    "\n",
    "    images = np.load(\"data/daynight.npz\")['data']\n",
    "\n",
    "There is also the **true labels** for each image in ['target']. **Obviously, don't use these in the clustering process!**.\n",
    "\n",
    "\n",
    "You should be able to cluster the images according to the time of day without using any labels. The raw pixel values can be used as features for clustering, but a more sensible approach is to summarise the image as a **color histogram**. \n",
    "\n",
    "This essentially splits the color space into coarse bins, and counts the occurence of each color type. You need to choose a value for $n$ (number of bins per channel) for the histogram; smaller numbers (like 3 or 4) are usually good.\n",
    "\n",
    "Make a function that can show the images and the corresponding cluster labels, to test how well clustering has worked; you might also see if there are additional meaningful clusters in the imagery.\n",
    "\n",
    "### Steps\n",
    "1. Load the imagery\n",
    "1. Check you can plot it (use `plt.imshow`)\n",
    "1. Create a set of features using `color_histogram()`\n",
    "1. Try clustering it and plotting the result\n",
    "1. Experiment with clustering algorithms and `color_histogram()` settings and see how this affects clustering performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_histogram(img, n):\n",
    "    \"\"\"Return the color histogram of the 2D color image img, which should have dtype np.uint8\n",
    "    n specfies the number of bins **per channel**. The histogram is computed in YUV space. \"\"\"\n",
    "    # compute 3 channel colour histogram using openCV\n",
    "    # we convert to YCC space to make the histogram better spaced\n",
    "    chroma_img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV) \n",
    "    # compute histogram and reduce to a flat array\n",
    "    return cv2.calcHist([chroma_img.astype(np.uint8)], channels=[0,1,2], mask=None, histSize=[n,n,n], ranges=[0,256,0,256,0,256]).ravel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load(\"data/daynight.npz\")['data']\n",
    "plt.imshow(cv2.cvtColor(images[1,:,:,:], cv2.COLOR_BGR2RGB))\n",
    "plt.grid(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"manifold\"></a>\n",
    "# Dimensional reduction\n",
    "A very common unsupervised learning task is *dimensional reduction*; taking a dataset with a dimension of $\\real^h$ and reducing to a dimension of $\\real^l$ which has fewer dimensions than $\\real^h$ but retains as much of the useful information as possible, for some definition of \"useful information\". The most common application is for **visualisation**, because humans are best at interpreting 2D data and struggle with higher dimensions.\n",
    "\n",
    "**Even 3D structure can be tricky for humans to get their heads around!**\n",
    "<img src=\"imgs/topologic.jpg\">\n",
    "\n",
    "Dimensional reduction can be thought of as a form of lossy compression -- finding a \"simpler\" representation of the data which captures its essential properties. This of course depends upon what the \"essential properties\" that we want to keep are, but generally we want to reject *noise* and keep non-random structure. We find a **subspace** that captures the meaningful variation of a dataset.\n",
    "\n",
    "One way of viewing this process is finding *latent variables*; variables we did not directly observe, but which are simple explanations of the ones we did observe. For example, if we measure a large number of weather measurements (rainfall, pressure, humidity, windspeed), these might be a very redundant representation of a few simple variables (e.g. is there a storm?). If features correlate or cluster in the measured data we can learn this structure *even without knowing training labels*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manifold learning\n",
    "One way of looking at this problem is learning a *manifold* on which the data lies (or lies close to). A *manifold* is a geometrical structure which is locally like a low-dimensional Euclidean space. Imagine data points lying on the surface of a sheet of paper crumpled into a ball, or a 1D filament or string tangled up in a 3D space. \n",
    "\n",
    "Manifold approaches attempt to automatically find these smooth embedded structures by examining the local structure of datapoints (often by analysing the nearest neighbour graph of points). This is more flexible than linear dimensional reduction as it can in theory unravel very complex or tangled datasets. \n",
    "\n",
    "However, the algorithms are usually approximate, they do not give guarantees that they will find a given manifold, and can be computationally intensive to run. \n",
    "\n",
    "<img src=\"imgs/isomap.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "One very simple method of dimensional reduction is *principal component analysis*. This is a linear method; in other words it finds rigid rotations and scalings of the data to project it onto a lower dimension. That is, it finds a matrix $A$ such that $y=Ax$ gives a mapping from $d$ dimensional $x$ to $d^\\prime$ dimensional $y$.\n",
    "\n",
    "The PCA algorithm effectively looks for the rotation that makes the dataset look \"fattest\" (maximises the variance), chooses that as the first dimension, then removes that dimension, rotates again to make it look \"fattest\" and repeats. Linear algebra makes it efficient to do this process in a single step by extracting the *eigenvectors* of the *covariance matrix*. \n",
    "\n",
    "PCA always finds a matrix $A$ such that $y = Ax$, where the dimension of $y<x$. PCA is exact and repeatable and very efficient, but it can only find rigid transformations of the data. This is a limitation of any linear dimensional reduction technique.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "digits = sklearn.datasets.load_digits()\n",
    "digit_data = digits.data\n",
    "\n",
    "# plot a single digit data element\n",
    "def show_digit(d):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    ax1 = fig.add_subplot(2,1,1)\n",
    "    ax1.imshow(d.reshape(8,8), cmap='gray', interpolation='nearest')\n",
    "    \n",
    "    ax2 = fig.add_subplot(2,1,2)\n",
    "    ax2.bar(np.arange(len(d)), d)\n",
    "    fig.subplots_adjust()\n",
    "    \n",
    "# show a couple of raw digits\n",
    "for i in range(3):\n",
    "    show_digit(digit_data[np.random.randint(0,1000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"PCA\")\n",
    "# apply principal component analysis\n",
    "pca = sklearn.decomposition.PCA(n_components=2).fit(digit_data)\n",
    "digits_2d = pca.transform(digit_data)\n",
    "\n",
    "# plot each digit with a different color\n",
    "plt.scatter(digits_2d[:,0], digits_2d[:,1], c=digits.target, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the projections\n",
    "One useful property of PCA is that we compute exactly how \"fat\" each of these learned dimensions were. The ratio of *explained variance* tells us how much each of the original variation in the dataset is captured by each learned dimension. \n",
    "\n",
    "If most of the variance is in the first couple of components, we know that a 2D representation will capture much of the original dataset. If the ratios of variance are spread out over many dimensions, we will need many dimensions to represent the data well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see how many dimensions we need to represent the data well using the eigenspectrum\n",
    "# here we show the first 32 components\n",
    "pca = sklearn.decomposition.PCA(n_components=32).fit(digit_data)\n",
    "plt.bar(np.arange(32), pca.explained_variance_ratio_)\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Proportion of variance explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of linearity\n",
    "One example where PCA does badly is the \"swiss roll dataset\" -- a plane rolled up into a spiral in 3D. This has a very simple structure; a simple plane with some distortion. But PCA can never unravel the spiral to find this simple explanation because it cannot be unravelled via a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "swiss_pos, swiss_val = sklearn.datasets.make_swiss_roll(800, noise=0.0)\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "# make a 3D figure\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(swiss_pos[:,0], swiss_pos[:,1], swiss_pos[:,2], c=swiss_val, cmap='gist_heat', s=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to learn this structure (which doesn't help much)\n",
    "plt.figure()\n",
    "pca = sklearn.decomposition.PCA(2).fit(swiss_pos)\n",
    "pca_pos = pca.transform(swiss_pos)\n",
    "plt.scatter(pca_pos[:,0], pca_pos[:,1], c=swiss_val, cmap='gist_heat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Nonlinear manifold learning\n",
    "Other approaches to dimensional reduction look at the problem in terms of learning a *manifold*. A *manifold* is a geometrical structure which is *locally like* a low-dimensional Euclidean space. Examples are the plane rolled up in the swiss roll, or a 1D \"string\" tangled up in a 3D space. \n",
    "\n",
    "Some manifold approaches attempt to automatically find these smooth embedded structures by examining the local structure of datapoints (often by analysing the nearest neighbour graph of points). This is more flexible than linear dimensional reduction as it can in theory unravel very complex or tangled datasets. \n",
    "\n",
    "However, the algorithms are usually approximate, they do not give guarantees that they will find a given manifold, and can be computationally intensive to run.\n",
    "\n",
    "A popular manifold learning algorithm is *ISOMAP* which uses nearest neighbour graphs to identify locally connected parts of a dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "np.random.seed(2018)\n",
    "swiss_pos, swiss_val = sklearn.datasets.make_swiss_roll(800, noise=0.0)\n",
    "isomap_pos = sklearn.manifold.Isomap(n_neighbors=10, n_components=2).fit_transform(swiss_pos)\n",
    "plt.scatter(isomap_pos[:,0], isomap_pos[:,1], c=swiss_val, cmap='gist_heat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# note that isomap is sensitive to noise!\n",
    "noisy_swiss_pos, swiss_val = sklearn.datasets.make_swiss_roll(800, noise=0.5)\n",
    "isomap_pos = sklearn.manifold.Isomap(n_neighbors=10, n_components=2).fit_transform(noisy_swiss_pos)\n",
    "plt.scatter(isomap_pos[:,0], isomap_pos[:,1], c=swiss_val, cmap='gist_heat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Self organising maps\n",
    "<a id=\"som\"></a>\n",
    "\n",
    "Self-organising maps are a nice half way house between clustering and manifold learning approaches. They create a dense \"net\" of clusters in the original (high-dimensional space), and force the cluster points to **also** lie in a low-dimensional space with local structure, for example, on a regular 2D grid. This maps a **discretized** low-dimensional space into the high-dimensional space.\n",
    "\n",
    "The algorithm causes the clusters have local smoothness in both the high and the low dimensional space; it does this by forcing cluster points on the grid to move closer (in the high-d space) to their neighbours (in the low-d grid).\n",
    "\n",
    "<img src=\"imgs/somtraining.png\"> [Image from https://en.wikipedia.org/wiki/Self-organizing_map]\n",
    "\n",
    "In other words: **clusters that are close together in the high-dimensional space should be close together in the low dimensional space**. This \"unravels\" high dimensional structure into a simple low-dimensional approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Self organising maps\n",
    "digits = sklearn.datasets.load_digits()\n",
    "digits.data -= 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge weave\n",
    "import som\n",
    "som = reload(som)\n",
    "som_map = som.SOM(48,48,64)\n",
    "som_map.learn(digits.data, epochs=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show SOM response for each *pixel* in the input image\n",
    "for v in [20,30,40,50]:\n",
    "    plt.figure()\n",
    "    plt.imshow(som_map.codebook[:,:,v], cmap=\"magma\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the SOM response for one node, across *all* pixels\n",
    "plt.imshow(som_map.codebook[20,20,:].reshape(8,8), cmap=\"gray\", interpolation=\"nearest\")\n",
    "plt.grid(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_codebook_images():\n",
    "    plt.figure(figsize=(32,32))\n",
    "    for i in range(0,48,2):\n",
    "        for j in range(0,48,2):\n",
    "            img = som_map.codebook[i,j,:].reshape(8,8)        \n",
    "            plt.imshow(img, cmap=\"gray\", extent=[i,i+2,j,j+2])\n",
    "    plt.xlim(0,48)\n",
    "    plt.ylim(0,48)\n",
    "    plt.axis(\"off\")\n",
    "show_codebook_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The U-Matrix\n",
    "\n",
    "One very nice aspect of the self-organsing map is that we can extract the **U-matrix** which captures how close together in the **high-dimensional space** points in the **low-dimensional** map are. This lets us see whether there are natural **partitions** in the layout; wrinkles in the layout that might be good clustering points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance\n",
    "\n",
    "def umatrix(codebook):\n",
    "    ## take the average HD distance to all neighbours within\n",
    "    ## certain radius in the 2D distance    \n",
    "    x_code, y_code = np.meshgrid(np.arange(codebook.shape[0]), np.arange(codebook.shape[1]))\n",
    "    hdmatrix = codebook.reshape(codebook.shape[0]*codebook.shape[1], codebook.shape[2])    \n",
    "    hd_distance = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(hdmatrix))**2\n",
    "    ld_distance = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(np.vstack([x_code.ravel(), y_code.ravel()]).T))\n",
    "    return np.mean(hd_distance * (np.logical_and(ld_distance>0,ld_distance<1.5)),axis=1).reshape(codebook.shape[0], codebook.shape[1])\n",
    "    \n",
    "plt.figure(figsize=(14,14))    \n",
    "um = umatrix(som_map.codebook)    \n",
    "show_codebook_images()\n",
    "plt.imshow(um, interpolation=\"nearest\", cmap=\"inferno\", alpha=0.75, extent=[0,48,48,0])\n",
    "\n",
    "plt.grid(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying to HCI\n",
    "These are standard machine learning techniques. How do we apply this practically to interaction? How can we solve the analysis -> synthesis problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"manifold_hci\"></a>\n",
    "## Laying out the keyboard vectors\n",
    "\n",
    "The keyboard vectors we captured at the start of this section look pretty noisy and unstructured. We can visualise them as a matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboard_data = np.load(\"data/rwo_Sun_Aug_26_20_47_40_2018.npy\")\n",
    "print(keyboard_data.shape)\n",
    "plt.imshow(keyboard_data.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ordering of elements is random, there is no spatial relation among keys visible. However, because there will be correlation between keys that were pressed close in time (because of the temporal smoothing, in this case), we would expect there to be some spatial information left. \n",
    "\n",
    "We can apply the self-organising map to this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "som_map = som.SOM(8,8,128)\n",
    "som_map.learn(keyboard_data, epochs=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can view the output live, as we move across the keyboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_som(k_vec):\n",
    "    # transform a keyboard vector to an output vector to display\n",
    "    z = som_map.score(np.zeros_like(k_vec), width=1.0) # remove constant part\n",
    "    result = np.clip(som_map.score(k_vec, width=1.0) -z, 0, 100)\n",
    "    return np.fliplr(result.T) * 3\n",
    "\n",
    "\n",
    "from key_display import key_tk\n",
    "import keyboard\n",
    "state = key_tk(transform_fn = live_som, shape=som_map.codebook.shape[0:2], alpha=0.8)\n",
    "%gui tk\n",
    "keyboard.restore_state(state) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did **not** train the system to map keys to physical locations. We simply captured the sensor stream, and identified the **control manifold** -- the space of sensor vectors that correspond to useful control signals. We were able to return to a 2D space and recover the spatial (or rather topological) structure of the vectors and use that as a useful control input. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"beard_pointer\"></a>\n",
    "## Learning a pointer\n",
    "\n",
    "\n",
    "### ISOMAP: The face-direction example\n",
    "<a id=\"isomap\"></a>\n",
    "A well known manifold learning algorithm is *ISOMAP* which uses nearest neighbour graphs to identify locally connected parts of a dataset. This examines local neighbor graphs to find an \"unraveling\" of the space to a 1D or 2D subspace, which can deal with very warped high-dimensional data, and doesn't get confused by examples like the swiss roll above (assuming parameters are set correctly!).\n",
    "\n",
    "Let's use ISOMAP (a local neighbours embedding approach) to build a real, working vision based interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a video of my head in different orientations\n",
    "face_frames = np.load(\"data/face_frames.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the video in opencv -- it's just a raw sequence of values\n",
    "# the video is 700 frames of 64x64 imagery\n",
    "frame_ctr = 0\n",
    "# play the video back \n",
    "while frame_ctr<face_frames.shape[1]:\n",
    "    frame = face_frames[:,frame_ctr].reshape(64,64)\n",
    "    cv2.imshow('Face video', cv2.resize(frame, (512,512), interpolation=cv2.INTER_NEAREST))\n",
    "    frame_ctr += 1\n",
    "    key = cv2.waitKey(5) & 0xff\n",
    "    if key  == 27:\n",
    "        break\n",
    "        \n",
    "# clean up\n",
    "cv2.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit isomap to the face data (this takes a few minutes)\n",
    "\n",
    "faces = face_frames.T\n",
    "np.random.seed(2018)\n",
    "isomap = sklearn.manifold.Isomap(n_neighbors=25)\n",
    "isomap.fit(faces)\n",
    "xy = isomap.transform(faces)\n",
    "orig_xy = np.array(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the following code just plots images on the plot without overlap\n",
    "overlaps = []\n",
    "\n",
    "def is_overlap(ra,rb):\n",
    "    P1X, P2X, P1Y, P2Y = ra\n",
    "    P3X, P4X, P3Y, P4Y = rb\n",
    "    \n",
    "    return not ( P2X <= P3X or P1X >= P4X or P2Y <= P3Y or P1Y >= P4Y )\n",
    "\n",
    "def overlap_test(r):\n",
    "    if any([is_overlap(r,rb) for rb in overlaps]):\n",
    "        return False\n",
    "    overlaps.append(r)\n",
    "    return True\n",
    "\n",
    "def plot_some_faces(xy, faces, thin=1.0, sz=8):\n",
    "    global overlaps\n",
    "    overlaps = []\n",
    "    q = sz/4\n",
    "    for i in range(len(xy)):\n",
    "        x, y = xy[i,0], xy[i,1]\n",
    "        image = faces[i,:].copy()\n",
    "        \n",
    "        if np.random.random()<thin:\n",
    "            for j in range(10):\n",
    "                x, y = xy[i,0], xy[i,1]\n",
    "                x += np.random.uniform(-q,q)\n",
    "                y += np.random.uniform(-q, q)\n",
    "                x *= q\n",
    "                y *= q\n",
    "                extent = [x, x+sz, y, y+sz]\n",
    "                if overlap_test(extent):                    \n",
    "                    img = image.reshape(64,64)\n",
    "                    img[:,0] = 1\n",
    "                    img[:,-1] = 1\n",
    "                    img[0,:] = 1\n",
    "                    img[-1,:] = 1                    \n",
    "                    plt.imshow(img, vmin=0, vmax=1, cmap=\"gray\",interpolation=\"lanczos\",extent=extent, zorder=100)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a 2D plot of the faces\n",
    "# tweak co-ordinates\n",
    "\n",
    "xy[:,0] = -orig_xy[:,0] / 2.5\n",
    "xy[:,1] = -orig_xy[:,1] \n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# plot the faces\n",
    "\n",
    "plot_some_faces(xy, faces, sz=8)\n",
    "\n",
    "# the axes correctly\n",
    "plt.xlim(np.min(xy[:,0])-10,np.max(xy[:,0])+10)\n",
    "plt.ylim(np.min(xy[:,1])-10,np.max(xy[:,1])+10)\n",
    "plt.gca().patch.set_facecolor('gray')\n",
    "plt.xlim(-70,70)\n",
    "plt.ylim(-70,70)\n",
    "plt.grid(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ctr = 0\n",
    "# play the video back, but show the projected dimension on the screen\n",
    "\n",
    "while frame_ctr<face_frames.shape[1]:\n",
    "    frame = face_frames[:,frame_ctr].reshape(64,64)\n",
    "    frame = (frame*256).astype(np.uint8)    \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "    xy = isomap.transform([face_frames[:,frame_ctr]])\n",
    "    cx, cy = 256, 256\n",
    "    s = 6\n",
    "    x,y = xy[0]\n",
    "    y = -y\n",
    "    resized_frame = cv2.resize(frame, (512,512), interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.circle(resized_frame, (int(cx-x*s), int(cy-y*s)), 10, (0,255,0), -1)\n",
    "    cv2.line(resized_frame, (cx,cy), (int(cx-x*s), int(cy-y*s)), (0,255,0))\n",
    "    cv2.imshow('Face video', resized_frame)\n",
    "    \n",
    "    frame_ctr += 1\n",
    "    key = cv2.waitKey(1) & 0xff\n",
    "    if key  == 27:\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"practical\"></a>\n",
    "    \n",
    "--------------------\n",
    "## Mapping UI controls to unsupervised structures\n",
    "<a id=\"mapping\"></a>\n",
    "\n",
    "The point of all of this is to find **control structures** in **sensor data**. That is, to find regularities in measured values that we could use to control a user interface.\n",
    "\n",
    "To do this, we need to map unsupervised structure onto the interface itself. We could at this point move to a supervised approach, now that we have likely candidates to target. But a simpler approach is just to hand-map unsupervised structure to controls.\n",
    "\n",
    "#### Clusters\n",
    "For example, if we have clustered a set of data (e.g. measurements of the joint angles of the hand), and extracted a set of fundamental poses, we can then create a mapping table from cluster indices to actions.\n",
    "\n",
    "|cluster | 1 | 2 | 3 | 4 |\n",
    "|-----------------------------------------------|\n",
    "|**action**  | confirm   | cancel    | increase  | decrease  |\n",
    "\n",
    "<img src=\"imgs/handposes.jpg\" width=\"400px\">\n",
    "\n",
    "\n",
    "#### Distance transform\n",
    "Sometimes it is useful to have some continuous elements in an otherwise discrete interface (e.g. to support animation on state-transitions). A useful trick is to use a **distance transform**, which takes a datapoint in the original measured space $D_H$ and returns the distances to all cluster centres. (`sklearn`'s `transform` function for certain clustering algorithms does this transformation for you)\n",
    "\n",
    "This could be used, for example, to find the top two candidates for a hand pose, and show a smooth transition between actions as the hand interpolates between them.\n",
    "\n",
    "The most obvious use of this is to **disable** any action when the distance to all clusters is too great. This implements a quiescent state and is part of solving the **Midas touch** problem; you only spend a small amount of time on a UI actively interacting and don't want to trigger actions all the time!\n",
    "\n",
    "## Manifolds\n",
    "\n",
    "In the continuous case, with a dimensional reduction approach, then the mapping can often be a simple transformation of the inferred manifold. This usually requires that the manifold be **oriented** correctly; for example, in the head pointing example, I adjusted the signs of the resulting 2D manifold to match the direction my nose points in. More generally, it might be necessary to apply a scaling or rotation of the output with a linear transform:\n",
    "\n",
    "$$ x_l = f(x_h)\\\\\n",
    "x_c = Ax_l,\n",
    "$$ where $x_l$ is the low-dimensional vector, $x_h$ is high dimensional sensor vector, $x_c$ is the vector (e.g. a cursor) we pass to the UI, and $A$ is a hand-tuned or learned transformation matrix.\n",
    "\n",
    "As an example, $A = \\begin{bmatrix}0 & 1 \\\\ -1 & 1\\end{bmatrix}$ exchanges the $x$ and $y$ co-ordinate and flips the sign of $y$.\n",
    "\n",
    "<img src=\"imgs/orienting.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more complex examples, we need need to learn a more sophsticated nonlinear mapping. For example, we might apply supervised learning to map output vectors to spatial locations. This might seem like cheating -- why bother with the unsupervised part?\n",
    "\n",
    "But the key insight is that we need vastly less training data to make this reliable. Moreover, we can factor the design process into:\n",
    "* capturing a representative dataset (e.g. rewarding the original)\n",
    "* estimating a good manifold  (e.g. using tSNE)\n",
    "* pinning it to useful actions (e.g. using a deep neural network)\n",
    "\n",
    "We can intervene at any part of these design processes and build on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Challenge\n",
    "<a id=\"challenge\"></a>\n",
    "In this practical, you will capture images from your webcam, and build a UI **control** using unlabeled data. Without providing **any** class labels or values, you have to build an interaction that can do \"something interesting\" from the image data. \n",
    "\n",
    "You have complete freedom to choose what the configuration space you want to use is; you could take images of your face or hands; take images of drawn figures; image an object rotating or moving across a surface; or anything else you want.\n",
    "\n",
    "As an illustrative example, the unsupervised approach could be used to image a soft drinks can at different rotations, and recover the rotation angle as an input (i.e. as a physical \"dial\").\n",
    "\n",
    "<img src=\"imgs/can.jpg\">\n",
    "\n",
    "The criterion is the most **interesting** but **functional** interface. The control can be discrete (using **clustering**) or continuous (using **manifold learning**). **You don't have to map the controls onto a real UI, just extract and visualise a useful signal from the image data**.\n",
    "\n",
    "The final system should be able to take a webcam image and output either a class or a (possibly $n$-dimensional) continuous value.\n",
    "\n",
    "## Tips\n",
    "\n",
    "* The webcam capture code is provided for you. `cam = Webcam()` creates a camera object and `img = cam.snap()` captures a single image from the first video device; if you have several, then you can use `cam = Webcam(1)` etc. to select the device to use. The result will be a $W\\times H\\times 3$ NumPy array, with colours **in the BGR order**.\n",
    "\n",
    "* You should resize your image (using `scipy.ndimage.zoom`) to something small (e.g. 32x48 or 64x64) so that the learning is feasible in the time available.\n",
    "\n",
    "* Your \"interface\" should probably show a 2D or 1D layout of the data in the training set, and have a mode where a new webcam image can be captured and plotted on the layout. You should consider colouring the data points by their attributes (e.g. cluster label) and/or showing some small images on the plot to get an idea of what is going on.\n",
    "\n",
    "* You can preprocess features as you like, but a good clustering/manifold learning algorithm will be able to capture much of the structure without this. **The simplicity of the processing applied will considered in judging!**; minimise the amount of hand-tweaking that you do.\n",
    "\n",
    "* Remember that some layout algorithms (e.g. t-SNE) are **unstable**. You may want to run the dimensional reduction several times and choose a good result, and use a repeatable random number seed (e.g. set it using `np.random.seed` or pass a custom `RandomState` to `sklearn`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple OpenCV image capture from the video device\n",
    "class Webcam(object):\n",
    "    def __init__(self, cam_id=0):\n",
    "        self.cap = cv2.VideoCapture(cam_id)        \n",
    "        \n",
    "    def snap(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        return frame    \n",
    "    \n",
    "# snap(), snap(), snap()..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More advanced unsupervised learning\n",
    "The algorithms we have seen so far are relatively old but well supported without tricky dependencies. There are many more modern approaches that can be used; unfortunately these are harder to setup for a one day course and often much slower to train. These include:\n",
    "\n",
    "* Deep autoencoder structures, which learn latent spaces by back propagating through a \"bottleneck layer\". [tSNE, for example can be cast as a deep learning structure](https://github.com/johnhw/tsne_demo) which is very flexible.\n",
    "![Paramteric tSNE](imgs/kyle_tsne_mnist.png)\n",
    "*[From: https://www.flickr.com/photos/kylemcdonald/25478228166 by Kyle McDonald]*\n",
    "\n",
    "* [Variational autoencoders (VAEs)](https://arxiv.org/abs/1606.05908), which are very powerful deep learning models for learning latent spaces\n",
    "\n",
    "* The outstanding [UMAP](https://umap-learn.readthedocs.io/en/latest/api.html) algorithm which is somewhat similar to tSNE, but often has better results in disentagling complex spaces. [See this talk for details](https://www.youtube.com/embed/nq6iPZVUxZU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
