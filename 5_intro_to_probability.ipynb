{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACM SIGCHI Summer School on Computational Interaction  \n",
    "### Inference, optimization and modeling for the engineering of interactive systems  \n",
    "#### 27th August - 1st September 2018  \n",
    "#### University of Cambridge, UK  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to probabilistic inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "plt.rc('figure', figsize=(8.0, 4.0), dpi=140)\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\newcommand{\\vec}[1]{{\\bf #1}} \n",
    "\\newcommand{\\real}{\\mathbb{R}}\n",
    "\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\vec{x}\n",
    "\\real\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic purpose\n",
    "This section will cover probabilistic **inference**. Rather than learning a single set of parameters by optimisation, we can infer probability distributions over possible models that might be compatible with our data.  \n",
    "\n",
    "\n",
    "Concretely, we'll use Monte Carlo sampling to make it simple and easy (if not very efficient) to work with probabilistic models. We will use these approaches to model **typing behaviour** at the keystroke level, and both make predictions given some data (\"how likely is it that this sequence was typed by user X?\") and quantify how much confidence we have in those models.\n",
    "\n",
    "\n",
    "\n",
    "## Outline for section A: Inferring typing behaviour\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this relevant for computational HCI?\n",
    "We will build **statistical models** of user behavior, and estimate parameters of that model from quantitative observations of data. \n",
    "\n",
    "This is **robust** (it appropriately represents uncertainty) and **generative** (it can simulate behaviour compatible with observations).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "<a id=\"bayesian\"> </a>\n",
    "# Random variables and distributions\n",
    "A **random variable** is a variable that can take on different values, but we do not know what value it has; i.e. one that is \"unassigned\". However, we have some knowledge which captures the possible states the variable could take on, and their corresponding probabilities. Probability theory allows us to manipulate random variables without having to assign them a specific value.\n",
    "\n",
    "A random variable is written with a capital letter, like $X$.\n",
    "\n",
    "A random variable might represent the outcome of dice throw (discrete); whether or not it is raining outside (discrete: binary); the height of person we haven't met yet (continuous); the position of a user's hand (continuous, multivariate);. \n",
    "\n",
    "## Distributions\n",
    "A **probability distribution** defines how likely different states of a random variable are. \n",
    "\n",
    "We can see $X$ as the the *experiment* and $x$ as the *outcome*, with a function mapping every possible outcome to a probability. We write $P(x)$ to mean the probability of $P(X=x)$ (note the case!).\n",
    "\n",
    "$$P(X=x),\\  \\text{the probability of random variable X taking on value x}\\\\\n",
    "P(X),\\  \\text{shorthand for probability of X=x }\\\\\n",
    "P(x),\\  \\text{shorthand for probability of specific value X=x }\\\\\n",
    "$$\n",
    "We can see an outcome as a random variable taking on a specific value i.e. $P(X=x)$. Note that we use $P(A)$ to mean the probability of **event** $A$, not the random variable $A$.\n",
    "\n",
    "### Discrete and continuous\n",
    "Random variables can be continuous (e.g. the height of a person) or discrete (the value showing on the face of a dice). \n",
    "\n",
    "* **Discrete variables** The distribution of a discrete variable is described with a **probability mass function** (PMF) which gives each outcome a specific value; imagine a dictionary mapping outcomes to probabilities. The PMF is usually written $f_X(x)$, where $P(X=x) = f_X(x)$.\n",
    "\n",
    "* **Continuous variables** A continuous variable has a **probability density function** (PDF) which specifies the spread of the probability as a *continuous function* $f_X(x)$. It is **not** the case that $P(X=x) = f_X(x)$ for PDFs.\n",
    "\n",
    "##### Integration to unity\n",
    "A probability mass function or probability density function *must* sum/integrate to exactly 1, as the random variable under consideration must take on *some* value. Every repetition of an experiment has exactly one outcome.\n",
    "\n",
    "$$\\sum_i f_X(x_i) = 1\\quad \\text{for PMFs of discrete RVs}$$\n",
    "$$\\int_x f_X(x)\\ dx = 1\\quad \\text{for PDFs of continuous RVs}$$\n",
    "---\n",
    "\n",
    "## PMF example: sum of dice rolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the PMF of the sum of two dice rolls\n",
    "def two_dice():\n",
    "    # form the sum of the cross product of these possibilities\n",
    "    roll_two = [i+j for i in range(1,7) for j in range(1,7)]\n",
    "    # now plot the histogram\n",
    "    pmf, edges, patches = plt.hist(roll_two, normed=True, bins=range(1,14))\n",
    "    print(\"Sum of PMF %.2f\" % np.sum(pmf)) # sum of probability should be *exactly* 1.0\n",
    "    plt.title(\"PMF of sum of 2d6 dice\")\n",
    "    plt.xlabel(\"Sum of rolls x\")\n",
    "    plt.ylabel(\"P(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples and sampling\n",
    "**Samples** are observed outcomes of an experiment; we will use the term **observations** synonymously. We can **sample** from a distribution; this means simulating outcomes according to the probability distribution of those variables.\n",
    "\n",
    "\n",
    "For example, we can sample from the sum of dice PMF by rolling two dice and summing the result. This is a sample or a draw from this distribution.\n",
    "\n",
    "\n",
    "For discrete random variables, this is easy: we simply produce samples by drawing each outcome according to its probability. For continuous variables, we need to use specific algorithms to draw samples according to a distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the PMF of the sum of two dice rolls\n",
    "def sample_two_dice():\n",
    "    roll_two = [i+j for i in range(1,7) for j in range(1,7)]    \n",
    "    pmf = np.histogram(roll_two, normed=True, bins=range(1,14))[0]\n",
    "    cmf  = np.cumsum(pmf) # cumulative sum of the amount of probability in each bin        \n",
    "    uniform_samples = np.random.uniform(0, 1, 200)\n",
    "    discrete_samples = np.digitize(uniform_samples, cmf) + 1 # compensate for bin starting on 1, not 0\n",
    "    \n",
    "    plt.hist(roll_two, bins=range(1,14), facecolor='C0', normed=True, alpha=0.2, label=\"Sampled histogram\")\n",
    "    plt.hist(discrete_samples, bins=range(1,14), facecolor='none', edgecolor='C1', linewidth=2, normed=True, label=\"True PMF\")\n",
    "    plt.legend()\n",
    "sample_two_dice()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability distribution functions (for continuous random variables)\n",
    "The PDF $f_X(x)$ of a random variable $X$ maps a value $x$ (which might be a real number, or a vector, or any other continuous value) to a single number, the density at the point. It is a function $\\real^N \\rightarrow \\real^+$, where $\\real^+$ is the positive real numbers.\n",
    "\n",
    "* While a PMF can have outcomes with a probability of at most 1, it is *not* the case that the maximum value of a PDF is $f_X(x) \\leq 1$ -- *just that the integral of the PDF be 1.*\n",
    "\n",
    "The value of the PDF at any point is **not** a probability, because the probability of a continuous random variable taking on any specific number must be zero. \n",
    "\n",
    "Instead, we can say that the probability of a continuous random variable $X$ lying in a range $[a,b]$ is:\n",
    "$$\\begin{equation} P(X \\in [a,b]) = (a < X < b)  = \\int_a^b f_X(x) \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF example: the normal disribution\n",
    "The most ubiquitous of all continuous PDFs is the **normal** or **Gaussian** distribution. It assigns probabilities to real values $x \\in {\\mathbb{R}}$ (in other words, a sample space consisting of all of the real numbers). It has a density given by the PDF:\n",
    "\n",
    "$$f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\, e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}$$\n",
    "\n",
    "\n",
    "We use a shorthand notation to refer to the distribution of continuous random variables:\n",
    "$$\\begin{equation}X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\end{equation},$$ \n",
    "which is read as \n",
    ">\"Random variable X is distributed as [N]ormal with mean $\\mu$ and variance $\\sigma^2$\"\n",
    "\n",
    "### Location and scale\n",
    "The normal distribution places most density close to its center $\\mu$ (the \"mean\"), with a spread defined by $\\sigma^2$ (the \"variance\"). This can be though of the **location** and **scale** of the density function. Most standard continuous random variable PDFs have a location (where density is concentrated) and scale (how spread out the density is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "# Plot the PDF of the normal distribution\n",
    "def plot_normal():\n",
    "    # plot the normal (Gaussian distibution) along with a set of points drawn from that distribution\n",
    "    x = np.linspace(-4,4,100)\n",
    "    y = stats.norm.pdf(x,0,1) # mean 0, std. dev. 1\n",
    "    plt.plot(x,y, label=\"PDF\")\n",
    "    plt.axhline(0, color='k', linewidth=0.2) # axis line\n",
    " \n",
    "    # mark the mean\n",
    "    plt.text(0, 0.51, '$\\mu$')\n",
    "    plt.axvline(0, color='r')\n",
    "    # highlight one std. dev. to the right\n",
    "    plt.axvspan(0,1, facecolor='b', alpha=0.1, label=\"1 std. dev.\")\n",
    "    plt.text(1.2, 0.3, '$\\sigma$')\n",
    "    # take 1000 random samples and scatter plot them\n",
    "    samples = stats.norm.rvs(0,1,1000)\n",
    "    plt.scatter(samples, np.full(samples.shape, .2), s=448, c='b', alpha=0.1, marker='|', label=\"Samples\")\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$P(x)$\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint, conditional, marginal\n",
    "\n",
    "The **joint probability** of two random variables is written $$P(X,Y)$$ and gives the probability that $X$ and $Y$ take the specific values *simultaneously* (i.e. $P(X=x) \\land P(Y=y)$). \n",
    "\n",
    "\n",
    "The **marginal probability** is the derivation of $P(X)$ from $P(X,Y)$ by integrating (summing) over all the possible outcomes of $Y$:\n",
    "$$P(X) = \\int_y P(X,Y=y) dy\\  \\text{for a PDF.}$$\n",
    "$$P(X) = \\sum_y P(X,Y=y)\\  \\text{for a PMF.}$$\n",
    "\n",
    "\n",
    "**Marginalisation** just means integration over one or more variables from a joint distribution: it *removes* those variables from the distribution.\n",
    "\n",
    "Two random variables are **independent** if the they do not have any dependence on each other. If this is the case then the joint distribution is just the product of the individual distributions:\n",
    "$P(X,Y) = P(X)P(Y).$ This is not true in the general case where the variables have dependence.\n",
    "\n",
    "The **conditional probability** of $X$ *given* $Y$ is written as $$P(X|Y)$$ and can be computed as $$\\begin{equation} P(X|Y) = \\frac{P(X,Y)}{P(Y)}. \\end{equation}$$ This tells us how likely $X$ is to occur *if we already know*  (or fix) the value of $Y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_marginal(cov):\n",
    "    # create an independent 2D normal distribution\n",
    "    x,y = np.meshgrid(np.linspace(-3,3,50), np.linspace(-3,3,50))\n",
    "    pos = np.empty(x.shape + (2,))\n",
    "    pos[:,:,0] = x\n",
    "    pos[:,:,1] = y\n",
    "    joint_pdf = scipy.stats.multivariate_normal.pdf(pos, [0,0], cov)\n",
    "    fig = plt.figure()\n",
    "    # plot the joint\n",
    "    ax = fig.add_subplot(2,2,1)\n",
    "    ax.axis('equal')\n",
    "    plt.title(\"Joint p(x,y)\")\n",
    "    ax.pcolor(x,y,joint_pdf, cmap='viridis')\n",
    "    # plot the marginals\n",
    "    ax = fig.add_subplot(2,2,3)\n",
    "    ax.axis('equal')\n",
    "    plt.title(\"Marginal $P(x) = \\int\\  P(x,y) dy$\")\n",
    "    ax.plot(x[0,:], np.sum(joint_pdf, axis=0))\n",
    "    ax = fig.add_subplot(2,2,2)\n",
    "    ax.axis('equal')\n",
    "    plt.title(\"Marginal $P(y) = \\int\\  P(x,y) dx$\")\n",
    "    ax.plot(np.sum(joint_pdf, axis=1), x[0,:])\n",
    "    # plot p(x|y)\n",
    "    ax = fig.add_subplot(2,2,4)\n",
    "    ax.axis('equal')\n",
    "    plt.title(\"Conditional $P(x|y) = \\\\frac{P(x,y)}{P(y)}$\")\n",
    "    marginal = np.tile(np.sum(joint_pdf, axis=0), (joint_pdf.shape[0],1))\n",
    "    ax.pcolor(x,y,joint_pdf/marginal, cmap='viridis')\n",
    "joint_marginal([[1,0],[0.5,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability theory and Bayesian inference\n",
    "\n",
    "#### Probability as a calculus of belief\n",
    "*Bayesians* treat probability as a **calculus of belief**; in this model of thought, probabilities are measures of degrees of belief. $P(A)=0$ means a belief that $A$ cannot be true and $P(A)=1$ is a belief that $A$ is absolutely certain.\n",
    "\n",
    "\n",
    "#### Probability as the optimal way of representing uncertainty\n",
    "Other representations of uncertainty are strictly inferior to probabilistic methods *in the sense that* a person, agent, computer placing \"bets\" on future events using probabilistic models has the best possible return out of all decision systems when there is uncertainty. \n",
    "\n",
    "*Bayesians* allow for belief in states to be combined and manipulated via the rules of probability. The key process in Bayesian logic is *updating of beliefs*. Given some *prior* belief (it's Glasgow, it's not likely to be sunny) and some new evidence (there seems to be a bright reflection inside) we can update our belief to calculate the *posterior* -- our new probability that it is sunny outside. Bayesian inference requires that we accept priors over events, i.e. that we must explicitly quantify our assumptions with probability distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior, likelihood, posterior, evidence\n",
    "\n",
    "We often want to know the probability of a some outcome $A$ given some other outcome $B$; that is $P(A|B)$. But we are often in the situation that we can only compute $P(B|A)$. \n",
    "\n",
    "In general $P(A|B) \\neq P(B|A);$ and the two expressions can be completely different. \n",
    "\n",
    "Typically, this type of problem occurs where we:\n",
    "* want to know the probability of some event given some *evidence* \n",
    "* but we only know the probability of the evidence given the event \n",
    "\n",
    "**Bayes' rule** gives a consistent way to invert the probability distribution:\n",
    "$$ \\begin{equation} P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \\end{equation}$$\n",
    "\n",
    "This follows directly from the axioms of probability. Bayes' Rule is a very important rule, and has some surprising results.\n",
    "\n",
    "* $P(A|B)$ is called the **posterior** -- what we want to know, or will know after the computation\n",
    "* $P(B|A)$ is called the **likelihood** -- how likely the event $A$ is to produce the evidence we see\n",
    "* $P(A)$ is the **prior**  -- how likely the event $A$ is regardless of evidence\n",
    "* $P(B)$ is the **evidence** -- how likely the evidence $B$ is regardless of the event.\n",
    "\n",
    "Bayes' rule gives a consistent rule to take some prior belief and combine it with observed data to estimate a new distribution which combines them.\n",
    "\n",
    "We often phrase this as some **hypothesis** $H$ we want to know, given some **data** $D$ we observe, and we write Bayes' Rule as:\n",
    "$$ \\begin{equation}P(H|D) = \\frac{P(D|H) P(H)}{P(D)} \\end{equation}$$\n",
    "\n",
    "(the probability of the hypothesis given the data) is equal to (the probability of the data given the hypothesis) times (the probability of the hypothesis) divided by (the probability of the data).\n",
    "\n",
    "In other words, if we want to work out how likely a hypothesis is to be true given observations, but we only know how likely we are to have seen those observations if that hypothesis *was* true, we can use Bayes' rule to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' rule for combining evidence\n",
    "Bayes' rule is the correct way to combine prior belief and observation to update beliefs. This can be used to \"learn\", where \"learning\" means updating a probability distribution based on observations. \n",
    "\n",
    "It has enormous applications anywhere uncertain information must be fused together, whether from multiple sources (e.g. sensor fusion) or over time (e.g. probabilistic filtering). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.stats\n",
    "\n",
    "def prior_posterior(prior_mean=0, prior_std=1, sonar_std=1, n=10, anim=False):\n",
    "    mean = prior_mean\n",
    "    std = prior_std\n",
    "    var = std*std\n",
    "    prior = scipy.stats.norm(mean,std)\n",
    "    evidence = scipy.stats.norm(1, 0.25)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    xs = np.linspace(-5,5,200)\n",
    "    ax.fill_between(xs, prior.pdf(xs), label=\"Prior belief\", alpha=0.3)\n",
    "    ax.fill_between(xs, evidence.pdf(xs), label=\"True generating PDF\", alpha=0.3)\n",
    "    \n",
    "    sample_var = sonar_std**2 # the *expected* variance of our observations\n",
    "    # note that changing this allows us to continously adjust our belief\n",
    "    # in our observations \n",
    "    ax.plot([0,0],[0,-0.1], 'c', alpha=0.7, label=\"Evidence\")\n",
    "    ax.plot([0,0],[0,-0.1], 'k:', alpha=0.7, label=\"Posterior belief\")\n",
    "    ax.set_title(\"Recursive Bayesian estimation\")\n",
    "    \n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"PDF $f_X(x)$\")\n",
    "    ax.axvline(1.0, label='True')\n",
    "    ax.legend()\n",
    "    for i in range(n):\n",
    "        \n",
    "        sample = evidence.rvs()\n",
    "        # single step update for a normal distribution    \n",
    "        mean = (var * sample + sample_var * mean) / (sample_var + var)\n",
    "        var = (var*sample_var) / (sample_var+var)     \n",
    "        \n",
    "        sample_pdf = scipy.stats.norm(sample, sonar_std).pdf\n",
    "        \n",
    "        # plot the sample and the resulting pdf\n",
    "        ax.plot([sample,sample],[0,-0.5], 'c', alpha=0.7)\n",
    "        if anim:\n",
    "            ax.plot(xs,-sample_pdf(xs), 'c', alpha=0.25)\n",
    "        ax.plot(xs, scipy.stats.norm(mean,np.sqrt(var)).pdf(xs), 'k:', alpha=0.25)\n",
    "        if anim:            \n",
    "            time.sleep(1.0)\n",
    "            fig.canvas.draw()\n",
    "        \n",
    "        \n",
    "    ax.fill_between(xs, scipy.stats.norm(mean,np.sqrt(var)).pdf(xs), color='g', label=\"Final posterior\", alpha=0.2)\n",
    "    ax.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "prior_posterior(0,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "prior_posterior(0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "prior_posterior(-3,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "prior_posterior(-3,0.5, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side note: Log probabilities\n",
    "\n",
    "The probability of multiple **independent** random variables taking on a set of values can be computed from the product:\n",
    "$$P(x,y,z) = P(x)P(y)P(z)$$\n",
    "and in general\n",
    "$$P(x_1, \\dots, x_n) = \\prod_{i=1}^{n} x_i$$\n",
    "\n",
    "We often have to have to compute such products, but to multiply lots of values $<1$ leads to numerical issues. Instead, we often prefer to manipulate *log probabilities*, which can be summed instead of multiplied:\n",
    "$$\\log P(x_1, \\dots, x_n) = \\sum_{i=1}^{n} \\log P(x_i)$$\n",
    "\n",
    "This is simply a numerical convenience. The **log-likelihood** is just $\\log P(B|A)$, and is often more convenient to work with than the raw likelihood. This avoids numerical underflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
